![](https://github.com/Microsoft/MCW-Template-Cloud-Workshop/raw/master/Media/ms-cloud-workshop.png 'Microsoft Cloud Workshops')

<div class="MCWHeader1">
Cosmos DB Real Time Advanced Analytics
</div>

<div class="MCWHeader2">
Whiteboard design session trainer guide
</div>

<div class="MCWHeader3">
January 2019
</div>

Information in this document, including URL and other Internet Web site references, is subject to change without notice. Unless otherwise noted, the example companies, organizations, products, domain names, e-mail addresses, logos, people, places, and events depicted herein are fictitious, and no association with any real company, organization, product, domain name, e-mail address, logo, person, place or event is intended or should be inferred. Complying with all applicable copyright laws is the responsibility of the user. Without limiting the rights under copyright, no part of this document may be reproduced, stored in or introduced into a retrieval system, or transmitted in any form or by any means (electronic, mechanical, photocopying, recording, or otherwise), or for any purpose, without the express written permission of Microsoft Corporation.

Microsoft may have patents, patent applications, trademarks, copyrights, or other intellectual property rights covering subject matter in this document. Except as expressly provided in any written license agreement from Microsoft, the furnishing of this document does not give you any license to these patents, trademarks, copyrights, or other intellectual property.

The names of manufacturers, products, or URLs are provided for informational purposes only and Microsoft makes no representations and warranties, either expressed, implied, or statutory, regarding these manufacturers or the use of the products with any Microsoft technologies. The inclusion of a manufacturer or product does not imply endorsement of Microsoft of the manufacturer or product. Links may be provided to third party sites. Such sites are not under the control of Microsoft and Microsoft is not responsible for the contents of any linked site or any link contained in a linked site, or any changes or updates to such sites. Microsoft is not responsible for webcasting or any other form of transmission received from any linked site. Microsoft is providing these links to you only as a convenience, and the inclusion of any link does not imply endorsement of Microsoft of the site or the products contained therein.

Â© 2019 Microsoft Corporation. All rights reserved.

Microsoft and the trademarks listed at <https://www.microsoft.com/en-us/legal/intellectualproperty/Trademarks/Usage/General.aspx> are trademarks of the Microsoft group of companies. All other trademarks are property of their respective owners.

**Contents**

<!-- TOC -->

- [Trainer information](#trainer-information)
  - [Role of the trainer](#role-of-the-trainer)
  - [Whiteboard design session flow](#whiteboard-design-session-flow)
  - [Before the whiteboard design session: How to prepare](#before-the-whiteboard-design-session-how-to-prepare)
  - [During the whiteboard design session: Tips for an effective whiteboard design session](#during-the-whiteboard-design-session-tips-for-an-effective-whiteboard-design-session)
- [Cosmos DB Real Time Advanced Analytics whiteboard design session student guide](#\insert-workshop-name-here-whiteboard-design-session-student-guide)
  - [Abstract and learning objectives](#abstract-and-learning-objectives)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study)
    - [Customer situation](#customer-situation)
    - [Customer needs](#customer-needs)
    - [Customer objections](#customer-objections)
    - [Infographic for common scenarios](#infographic-for-common-scenarios)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution)
  - [Step 3: Present the solution](#step-3-present-the-solution)
  - [Wrap-up](#wrap-up)
  - [Additional references](#additional-references)
- [Cosmos DB Real Time Advanced Analytics whiteboard design session trainer guide](#\insert-workshop-name-here-whiteboard-design-session-trainer-guide)
  - [Step 1: Review the customer case study](#step-1-review-the-customer-case-study-1)
  - [Step 2: Design a proof of concept solution](#step-2-design-a-proof-of-concept-solution-1)
  - [Step 3: Present the solution](#step-3-present-the-solution-1)
  - [Wrap-up](#wrap-up-1)
  - [Preferred target audience](#preferred-target-audience)
  - [Preferred solution](#preferred-solution)
  - [Checklist of preferred objection handling](#checklist-of-preferred-objection-handling)
  - [Customer quote (to be read back to the attendees at the end)](#customer-quote-to-be-read-back-to-the-attendees-at-the-end)

<!-- /TOC -->

# Trainer information

Thank you for taking time to support the whiteboard design sessions as a trainer!

## Role of the trainer

An amazing trainer:

- Creates a safe environment in which learning can take place.

- Stimulates the participant's thinking.

- Involves the participant in the learning process.

- Manages the learning process (on time, on topic, and adjusting to benefit participants).

- Ensures individual participant accountability.

- Ties it all together for the participant.

- Provides insight and experience to the learning process.

- Effectively leads the whiteboard design session discussion.

- Monitors quality and appropriateness of participant deliverables.

- Effectively leads the feedback process.

## Whiteboard design session flow

Each whiteboard design session uses the following flow:

**Step 1: Review the customer case study (15 minutes)**

**Outcome**

Analyze your customer's needs.

- Customer's background, situation, needs and technical requirements

- Current customer infrastructure and architecture

- Potential issues, objectives and blockers

**Step 2: Design a proof of concept solution (60 minutes)**

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

- Determine your target customer audience.

- Determine customer's business needs to address your solution.

- Design and diagram your solution.

- Prepare to present your solution.

**Step 3: Present the solution (30 minutes)**

**Outcome**

Present solution to your customer:

- Present solution

- Respond to customer objections

- Receive feedback

**Wrap-up (15 minutes)**

- Review preferred solution

## Before the whiteboard design session: How to prepare

Before conducting your first whiteboard design session:

- Read the Student guide (including the case study) and Trainer guide.

- Become familiar with all key points and activities.

- Plan the point you want to stress, which questions you want to drive, transitions, and be ready to answer questions.

- Prior to the whiteboard design session, discuss the case study to pick up more ideas.

- Make notes for later.

## During the whiteboard design session: Tips for an effective whiteboard design session

**Refer to the Trainer guide** to stay on track and observe the timings.

**Do not expect to memorize every detail** of the whiteboard design session.

When participants are doing activities, you can **look ahead to refresh your memory**.

- **Adjust activity and whiteboard design session pace** as needed to allow time for presenting, feedback, and sharing.

- **Add examples, points, and stories** from your own experience. Think about stories you can share that help you make your points clearly and effectively.

- **Consider creating a "parking lot"** to record issues or questions raised that are outside the scope of the whiteboard design session or can be answered later. Decide how you will address these issues, so you can acknowledge them without being derailed by them.

**\*Have fun**! Encourage participants to have fun and share!\*

**Involve your participants.** Talk and share your knowledge but always involve your participants, even while you are the one speaking.

**Ask questions** and get them to share to fully involve your group in the learning process.

**Ask first**, whenever possible. Before launching into a topic, learn your audience's opinions about it and experiences with it. Asking first enables you to assess their level of knowledge and experience, and leaves them more open to what you are presenting.

**Wait for responses**. If you ask a question such as, "What's your experience with (fill in the blank)?" then wait. Do not be afraid of a little silence. If you leap into the silence, your participants will feel you are not serious about involving them and will become passive. Give participants a chance to think, and if no one answers, patiently ask again. You will usually get a response.

# Cosmos DB Real Time Advanced Analytics whiteboard design session student guide

## Abstract and learning objectives

\[insert what is trying to be solved for by using this workshop. . . \]

## Step 1: Review the customer case study

**Outcome**

Analyze your customer's needs.

Timeframe: 15 minutes

Directions: With all participants in the session, the facilitator/SME presents an overview of the customer case study along with technical tips.

1.  Meet your table participants and trainer.

2.  Read all of the directions for steps 1-3 in the student guide.

3.  As a table team, review the following customer case study.

### Customer situation

Woodgrove Bank, who provides payment processing services for commerce, is looking to design and implement a PoC of an innovative fraud detection solution. They know from experience and through contacts in the financial industry that there is a constant arms race between fraudsters and banks. Thanks to increasingly powerful and easily accessible technology, financial crime is on the rise. Payment processing companies, like Woodgrove Bank, and their merchant customers risk financial losses due to fraud. They also risk fines from failing to detect or even prevent criminal acts like money laundering or terror financing. In addition, other forms of fraud are on the rise, like ATM fraud, card transaction fraud, payment fraud, and more. In the most basic terms, online fraud is committed when an unauthorized user impersonates another user by taking over their account, using malware, or hijacking internet sessions. When dealing with millions of transactions, it is both crucial and challenging to detect and monitor fraud in real-time across all transactions. Doing so helps prevent additional losses and detect widespread attacks.

Another motive for rapidly prototyping a real-time fraud detection system is that Woodgrove forecasts reaching over USD \$10 Billion in assets over the upcoming fiscal year, placing them within the stricter regulatory purview of institutions classified by the US government as "big banks". This means that they will be subject to regulatory fines over and above the fraud loss, putting their business at greater risk. Their hope is that building a prototype will help them de-risk certain aspects of the solution they are unsure about, such as whether their streaming data can be processed rapidly enough for real-time detection. They also want to ensure that their historical data is sufficient for training a solid machine learning model and that they have a path forward to constantly retrain and redeploy the model for optimal performance. Finally, they would like to balance reducing false positives with increasing the detection of real fraud. The biggest issue with false positives is that it negatively impacts customer satisfaction. The customer's account could be suspended during investigation, including any pending transactions.

With these challenges in mind, Woodgrove Bank wants to use AI to break the tradeoff between false positive and false negative errors. They hope a well-trained model will also reduce the chance of missing true positives, and do so in a way that is highly performant and globally available. Woodgrove's vision is to offer this capability in the form of new services to their merchant customers. As stated before, their customers are located around the world, and the right solutions for them would minimize any latencies experienced using their service by distributing as much of the solution as possible, as closely as possible, to the regions in which their customers use the service.

In addition to real-time fraud detection, Woodgrove Bank would like to periodically evaluate data, customer by customer, and offer a pre-scored data set to merchants on-demand that scores a customer's fraud likelihood based on their transaction history. This "trust list", as well as the real-time scoring model, needs to be made globally available and able to handle varying levels of consumer demand.

### Customer needs

1.  Need to provide fraud detection services to our merchant customers, using incoming real-time payment transaction data to provide early warning of fraudulent activity.

2.  We need guidance on how to choose the right ML algorithm, periodically retrain our model, and redeploy the best version of the retrained model for real-time scoring on our web applications.

3.  We would like to schedule batch scoring using our trained model, and make that data globally available in regions closest to our customers through our web applications.

4.  Need to be able to store data from streaming sources into long-term storage, without interfering with jobs reading the data set.

5.  We would like to use a standard platform that supports our near-term data pipeline needs while providing a long-term standard for data science, data engineering, and development.

6.  Need to be able to have a top-level view of fraud trends across the globe.

### Customer objections

1.  We've found it challenging in the past to deal with both streaming data and long-term storage in a unified way, making it difficult to perform inserts, updates, and deletes to logical tables, having to manually maintain separate transaction logs. Over time, queries became slower because of having so many small files. How do we address these issues?

2.  We are worried about storing secrets, like connection strings, within a notebook anyone can access. We want a centralized way to store these secrets that are accessible across services to cut down on redundancy.

3.  Properly selecting the right algorithm and training a model using the optimal set of parameters can take a lot of time. Is there a way to speed up this process?

### Infographic for common scenarios

\[insert your custom workshop content here . . . \]

## Step 2: Design a proof of concept solution

**Outcome**

Design a solution and prepare to present the solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 60 minutes

**Business needs**

Directions: With all participants at your table, answer the following questions and list the answers on a flip chart:

1.  Who should you present this solution to? Who is your target customer audience? Who are the decision makers?

2.  What customer business needs do you need to address with your solution?

**Design**

Directions: With all participants at your table, respond to the following questions on a flip chart:

_High-level architecture_

1.  Without getting into the details (the following sections will address the particular details), diagram your initial vision for handling the top-level requirements for payment fraud detection, including stream capture and processing, long-term storage, model training, global distribution of the model for real-time scoring and of the pre-scored fraud data, and dashboards.

_Globally distributed data_

1.  Which data storage service would you recommend using that can minimize access latency to globally distributed pre-scored fraud data? Be specific about how data is replicated.

2.  How does your chosen service handle scaling to meet varying levels of demand across different regions?

3.  The customer also wants to globally distribute access to trained machine learning models used for real-time fraud detection. How would you architect your solution to meet this requirement?

_Data ingest_

1.  What are your recommended options for ingesting payment transaction events as they occur in a scalable way that can be easily processed while maintaining event order with no data loss?

2.  Given the technical and business requirements at hand, is it best to narrow your options to one platform, or would you combine services for ingesting this data?

_Data pipeline processing_

1.  Woodgrove Bank indicated that they would like a unified way to process both streaming data and batch data on a platform that can also support their data science, data engineering, and development needs. Which platform would you recommend, and why?

2.  They are also concerned about difficulties they have had in the past with performing both inserts and updates to long-term storage while processing streaming and batch data. How will your chosen platform cope with this challenge while optimizing file storage, avoiding the degradation in query performance due to many small files?

3.  How will your chosen data processing platform connect to and process data from your chosen data ingest solution for streaming data?

4.  The customer is concerned about being able to protect secrets, like service account keys and connection strings. How do you propose storing and providing access to these secrets within the selected data pipeline processing platform?

_Long-term data storage_

1.  As incoming data is processed, refined, and scored, all of the transactions need to be persisted to long-term storage for analysis, model training and validation, and reporting. This storage needs to handle long-term growth, be fast enough to rapidly ingest new data while simultaneously handling reads against the same data set without interference, and act as a reliable data source for dashboards and reports. Which is your recommended long-term data storage solution, keeping in mind its role within your selected data pipeline processing platform?

2.  How do you ensure your data is continuously optimized within your chosen long-term data storage solution, given the requirements to store inserts, updates, and deletes while avoiding generating very small, un-optimized files?

3.  Woodgrove Bank wants to retain all raw data (bronze), then parse that data into query tables (silver) which can be joined with dimension tables, such as account information. They also would like to have summary tables (gold) containing business-level aggregates used for their dashboards and reports. How would you support these requirements in your long-term storage solution?

_Model training and deployment_

1.  Describe how your chosen data processing platform will support machine learning model training and deployment. The model will need to be trained on and validated against historical payment transaction data that includes known fraudulent transactions.

2.  What are some processes that can be put in place to support a collaborative approach to producing and deploying the model?

3.  How do you propose deploying the trained model in a way that is scalable, globally accessible, and supports redeploying new versions of the model with little to no downtime? Can these steps be performed within the same data processing platform in which the model is re-trained? How will you support model versioning?

4.  How will you address Woodgrove Bank's desire to simplify the process of running and tracking model experiments through hyperparameter tuning, and selecting the best model based on those experiments?

5.  How will you schedule regular batch scoring of fraud data using the trained model, and make that data available to Woodgrove Bank's web applications at a global scale?

_Dashboards and reporting_

1.  Woodgrove Bank's business analysts would like to have a set of dashboards they can monitor that provide real-time views of fraud trends at a global scale. Thinking back to how your proposed solution provides a set of summary (gold) tables containing business-level aggregates, what do you propose using to meet this requirement? Be specific about how this solution will be put in place and which features it supports.

2.  How do you propose giving access to this same data to Woodgrove Bank's data scientists and data engineers within the data processing environment wherein they can craft complex queries and data visualizations?

**Prepare**

Directions: With all participants at your table:

1.  Identify any customer needs that are not addressed with the proposed solution.

2.  Identify the benefits of your solution.

3.  Determine how you will respond to the customer's objections.

Prepare a 15-minute chalk-talk style presentation to the customer.

## Step 3: Present the solution

**Outcome**

Present a solution to the target customer audience in a 15-minute chalk-talk format.

Timeframe: 30 minutes

**Presentation**

Directions:

1.  Pair with another table.

2.  One table is the Microsoft team and the other table is the customer.

3.  The Microsoft team presents their proposed solution to the customer.

4.  The customer makes one of the objections from the list of objections.

5.  The Microsoft team responds to the objection.

6.  The customer team gives feedback to the Microsoft team.

7.  Tables switch roles and repeat Steps 2-6.

## Wrap-up

Timeframe: 15 minutes

Directions: Tables reconvene with the larger group to hear the facilitator/SME share the preferred solution for the case study.

## Additional references

|                 |           |
| --------------- | :-------: |
| **Description** | **Links** |
|                 |           |
|                 |           |
|                 |           |
|                 |           |

# Cosmos DB Real Time Advanced Analytics whiteboard design session trainer guide

## Step 1: Review the customer case study

- Check in with your table participants to introduce yourself as the trainer.

- Ask, "What questions do you have about the customer case study?"

- Briefly review the steps and timeframes of the whiteboard design session.

- Ready, set, go! Let the table participants begin.

## Step 2: Design a proof of concept solution

- Check in with your tables to ensure that they are transitioning from step to step on time.

- Provide some feedback on their responses to the business needs and design.

  - Try asking questions first that will lead the participants to discover the answers on their own.

- Provide feedback for their responses to the customer's objections.

  - Try asking questions first that will lead the participants to discover the answers on their own.

## Step 3: Present the solution

- Determine which table will be paired with your table before Step 3 begins.

- For the first round, assign one table as the presenting team and the other table as the customer.

- Have the presenting team present their solution to the customer team.

  - Have the customer team provide one objection for the presenting team to respond to.

  - The presentation, objections, and feedback should take no longer than 15 minutes.

  - If needed, the trainer may also provide feedback.

## Wrap-up

- Have the table participants reconvene with the larger session group to hear the facilitator/SME share the following preferred solution.

## Preferred target audience

\[insert your custom workshop content here . . . \]

## Preferred solution

_High-level architecture_

1.  Without getting into the details (the following sections will address the particular details), diagram your initial vision for handling the top-level requirements for payment fraud detection, including stream capture and processing, long-term storage, model training, global distribution of the model for real-time scoring and of the pre-scored fraud data, and dashboards.

    ![The Solution diagram is described in the text following this diagram.](../Media/outline-architecture.png 'Solution diagram')

    The solution begins with streaming payment transaction events...

    > **Note**: The preferred solution is only one of many possible, viable approaches.

_Globally distributed data_

1.  Which data storage service would you recommend using that can minimize access latency to globally distributed pre-scored fraud data? Be specific about how data is replicated.

    Azure Cosmos DB is well-suited for delivering large amounts of data in a fast and reliable way through data centers around the world. The fact that it supports multiple models (documents, graphs, key-value, column-family) without requiring schemas provides the flexibility of choice and changing data feature requirements. In addition, there is a connector available for reading and writing to Cosmos DB from Spark clusters, making it a good candidate for both data ingest and as a data serving layer.

    It is a simple process to add or remove geographical regions associated with a Cosmos DB database at any time with a few clicks, or programmatically through a single API call. Because Cosmos DB automatically indexes the data stored within a Cosmos container (database) upon ingestion, users can query the data without having to deal with a schema or the complications of index management in a globally distributed setup.

    When Woodgrove Bank configures Cosmos DB, they should take good care to select an appropriate partition key for their data. They should select a partition key which provides even distribution of storage and throughput (req/sec) at any given time to avoid storage and performance bottlenecks (for instance, account number or region). This key should be present in the bulk of queries for read-heavy scenarios to avoid excessive fan-out, since each document with a specific partition key value belongs to the same logical partition, and is therefore transparently placed in the same physical partition. Each physical partition is replicated across geographical regions, resulting in global distribution.

2.  How does your chosen service handle scaling to meet varying levels of demand across different regions?

    In Azure Cosmos DB, provisioned throughput is represented as request units/second (RUs). RUs measure the cost of both read and write operations against your Cosmos container. Because Cosmos DB is designed with transparent horizontal scaling and multi-master replication, you can very quickly and easily increase or decrease the number of RUs to handle thousands to hundreds of millions of requests per second around the globe with a single API call.

    When you set a number of RUs for a container, Cosmos DB ensures that those RUs are available in each region associated with your Cosmos DB account. When you scale out the number of regions by adding a new one, Cosmos will automatically provision the 'R' RUs in the newly added region. You cannot selectively assign different RUs to a specific region. These RUs are provisioned for a container (or database) for all associated regions.

    Woodgrove Bank should be aware of the available consistency levels of Azure Cosmos DB, and the potential tradeoffs between read consistency, availability, latency, and throughput. As a general rule of thumb, you can get about 2x read throughput for session, consistent prefix, and eventual consistency models compared to bounded staleness or strong consistency.

3.  The customer also wants to globally distribute access to trained machine learning models used for real-time fraud detection. How would you architect your solution to meet this requirement?

    Use Azure Machine Learning service and the Azure Machine Learning SDK to host a trained machine learning model and automatically deploy the model to an Azure Kubernetes Service (AKS) cluster. Creating the AKS cluster is a one-time process for your workspace, whereafter you can reuse it for multiple deployments as the model gets updated through re-training. The basic steps are as follows:

    1.  Register the model in the workspace model registry.

    2.  Build a Docker image, including:

        - Download the registered model from the registry.
        - Create a dockerfile, with a Python environment based on the dependencies you specify in the environment yaml file.
        - Add your model files and the scoring script you supply in the dockerfile.
        - Build a new Docker image using the dockerfile.
        - Register the Docker image with the Azure Container Registry associated with the workspace.

    3.  Deploy the Docker image to Azure Kubernetes Service (AKS).

    4.  Start up a new container (or containers) in AKS.

    The above can be done through a series of scripts for automation. To deploy globally, modify the script to deploy to AKS clusters hosted within different regions.

_Data ingest_

1.  What are your recommended options for ingesting payment transaction events as they occur in a scalable way that can be easily processed while maintaining event order with no data loss?

    There are a couple of options in Azure for ingesting real-time streaming payment transactions. Which one you choose will depend on various factors, including the rate of flow (how many transactions/second), data source and compatibility, and long-term storage needs:

    1.  **Event Hubs** is a Big Data streaming platform and event ingestion service, capable of ingesting millions of events per second. It supports multiple consumers (event processors), and is automatically scalable. Event Hubs is a good candidate for this architecture for the following reasons:

        - Fully managed PaaS with little configuration or management overhead.
        - Highly scalable to process millions of events per second. Use the Auto-inflate feature to automatically scale the number of throughput units to meet usage needs.
        - Contains an optional Apache Kafka endpoint, allowing for event processing from existing Kafka-based applications. This also allows for simple integration with Apache Spark clusters, such as those hosted in Azure Databricks.
        - Simultaneously supports real-time and batch processing through Event Hubs Capture. This feature allows one to easily capture and store all events in their raw form to either Azure Blob storage or Azure Data Lake Store for long-term retention and micro-batch processing.
        - Event publishers (systems sending payment transaction data) can publish events using HTTPS, AMQP 1.0, or Apache Kafka 1.0 and above.
        - Event consumers can process streams using .NET, Java, Python, Go, or Node.js.

    Things to be cautious about are:

        -  Event Hubs guarantees consistency and event ordering _per partition_. Since it is important to keep payment transactions in order when processing them, you can guarantee ordering by setting a partition key on the event, or use a `PartitionSender` object to only send events to a certain partition. This has scaling implications if you plan to use more than one partition. It also has uptime implications if the partition you are trying to send to is unavailable. If no partition is defined, then the data is distributed across available partitions. You may choose to ensure ordering by region or merchant, as an example, by creating a partition for the region or merchant. The potential downside to this approach is finite number of available partitions for a given event hub (32 max by default, higher via quota increase request). Another option is to aggregate events within the processing application by time-stamping the event with a custom sequence number. This requires state to be kept within the processing application.

    2.  **Azure Cosmos DB change feed** outputs a sorted list of documents that were changed in the order in which they were modified or inserted. Like Event Hubs, the change feed output can be distributed across one or more consumers for parallel processing. Azure Cosmos DB change feed is a good candidate for this architecture for the following reasons:

        - Fully managed PaaS with little configuration or management overhead.
        - Cosmos DB is highly scalable, and is already being used to store pre-scored fraud data.
        - An Apache Spark connector is available, allowing Azure Databricks clusters to directly access the change feed with very little code.
        - Cosmos DB with change feed enabled acts as both a raw data store for batch processing and stream processing.
        - Event publishers can publish events to Cosmos DB using .NET, Java, Node.js, and Python, using a number of APIs, such as SQL, Cassandra, MongoDB, Gremlin, and Azure Table Storage. However, please note that change feed is currently only supported by the SQL and Gremlin APIs.
        - Cosmos DB is globally accessible across many Azure regions, bringing it closer to distributed event publishers and consumers.
        - In a multi-region Azure Cosmos account, if a write-region fails over, change feed will work across the manual failover operation and it will be contiguous.

    Things to be cautious about are:

        -  Similar to Event Hubs, feed item ordering is guaranteed per logical partition key. There is no guaranteed order across the partition key values. Also similar to Event Hubs, changes are available in parallel across all logical partition keys of a container, allowing for parallel processing by multiple consumers. As discussed in the previous section (globally distributed data), choosing an appropriate partition key for Cosmos DB is a critical step for ensuring balanced reads and writes, scaling, and, in this case, in-order change feed processing per partition. While there are no limits, per se, on the number of logical partitions, a single logical partition is allowed an upper limit of 10 GB of storage. Logical partitions cannot be split across physical partitions. For the same reason, if the partition key chosen is of bad cardinality, we could potentially have skewed storage distribution. For instance, if one logical partition becomes fatter faster than the others and hits the maximum limit of 10 GB, while the others are nearly empty, the physical partition housing the maxed out logical partition cannot split and could cause an application downtime.

2.  Given the technical and business requirements at hand, is it best to narrow your options to one platform, or would you combine services for ingesting this data?

    While it is certainly possible to combine Azure Cosmos DB and Event Hubs for data ingest, this may result in unnecessary complexity, especially considering how closely their features (and challenges) align. For instance, it is possible to ingest all data into Cosmos DB and send events to Event Hubs through an intermediary event processor, such as Azure functions, in order to further process the event hub data downstream by consumers that have no way to use Cosmos DB's change feed. However, this additional layer of abstraction is not necessary for Woodgrove Bank's scenario. All things considered, the best approach is to select one after weighing the pros and cons of each. Remember, you decision should be based on the following factors: the rate of flow (how many transactions/second), data source and compatibility, and long-term storage needs. If the payment processors sending the transaction data can be more easily adapted to send to one service over the other, then the more compatible service may garner higher preference.

    Both services are certainly capable of acting as the ingestion service. Since ordering of transactions is of such high importance, the restrictions on the number of partitions Event Hubs can have tilts the scale toward Cosmos DB change feed. This limitation can be worked around with Event Hubs, but the level of effort to do so may not be worth it in the long run. In this case, we will recommend Azure Cosmos DB for ingest.

_Data pipeline processing_

1.  Woodgrove Bank indicated that they would like a unified way to process both streaming data and batch data on a platform that can also support their data science, data engineering, and development needs. Which platform would you recommend, and why?

2.  They are also concerned about difficulties they have had in the past with performing both inserts and updates to long-term storage while processing streaming and batch data. How will your chosen platform cope with this challenge while optimizing file storage, avoiding the degradation in query performance due to many small files?

3.  How will your chosen data processing platform connect to and process data from your chosen data ingest solution for streaming data?

4.  The customer is concerned about being able to protect secrets, like service account keys and connection strings. How do you propose storing and providing access to these secrets within the selected data pipeline processing platform?

_Long-term data storage_

1.  As incoming data is processed, refined, and scored, all of the transactions need to be persisted to long-term storage for analysis, model training and validation, and reporting. This storage needs to handle long-term growth, be fast enough to rapidly ingest new data while simultaneously handling reads against the same data set without interference, and act as a reliable data source for dashboards and reports. Which is your recommended long-term data storage solution, keeping in mind its role within your selected data pipeline processing platform?

2.  How do you ensure your data is continuously optimized within your chosen long-term data storage solution, given the requirements to store inserts, updates, and deletes while avoiding generating very small, un-optimized files?

3.  Woodgrove Bank wants to retain all raw data (bronze), then parse that data into query tables (silver) which can be joined with dimension tables, such as account information. They also would like to have summary tables (gold) containing business-level aggregates used for their dashboards and reports. How would you support these requirements in your long-term storage solution?

_Model training and deployment_

1.  Describe how your chosen data processing platform will support machine learning model training and deployment. The model will need to be trained on and validated against historical payment transaction data that includes known fraudulent transactions.

2.  What are some processes that can be put in place to support a collaborative approach to producing and deploying the model?

3.  How do you propose deploying the trained model in a way that is scalable, globally accessible, and supports redeploying new versions of the model with little to no downtime? Can these steps be performed within the same data processing platform in which the model is re-trained? How will you support model versioning?

4.  How will you address Woodgrove Bank's desire to simplify the process of running and tracking model experiments through hyperparameter tuning, and selecting the best model based on those experiments?

5.  How will you schedule regular batch scoring of fraud data using the trained model, and make that data available to Woodgrove Bank's web applications at a global scale?

_Dashboards and reporting_

1.  Woodgrove Bank's business analysts would like to have a set of dashboards they can monitor that provide real-time views of fraud trends at a global scale. Thinking back to how your proposed solution provides a set of summary (gold) tables containing business-level aggregates, what do you propose using to meet this requirement? Be specific about how this solution will be put in place and which features it supports.

2.  How do you propose giving access to this same data to Woodgrove Bank's data scientists and data engineers within the data processing environment wherein they can craft complex queries and data visualizations?

## Checklist of preferred objection handling

\[insert your custom workshop content here . . . \]

## Customer quote (to be read back to the attendees at the end)

\[insert your custom workshop content here . . . \]
